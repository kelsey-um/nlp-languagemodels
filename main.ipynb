{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as ET #lxml is faster\n",
    "import glob #to use multiple files\n",
    "import json #to save model\n",
    "import random\n",
    "import re #regular expressions\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count:  11503\n"
     ]
    }
   ],
   "source": [
    "#one file\n",
    "\n",
    "tree = ET.parse('xmlfiles/A1E.xml')\n",
    "root = tree.getroot()\n",
    "# print(root)\n",
    "\n",
    "textPartial=[]\n",
    "\n",
    "for s in root.iter('s'):\n",
    "    textPartial.append('<s>')\n",
    "    for elem in s.findall('*'):\n",
    "\n",
    "        if elem.text != None:    \n",
    "            textPartial.append(elem.text)\n",
    "    \n",
    "    textPartial.append('</s>')\n",
    "\n",
    "print(\"Word count: \",len(textPartial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count:  1174283\n"
     ]
    }
   ],
   "source": [
    "#all files\n",
    "\n",
    "files = glob.glob('xmlfiles/*.xml') #to open all xml files\n",
    "\n",
    "textFull = []\n",
    "iter = 0\n",
    "\n",
    "for fileName in files: # iterating through all files\n",
    "\n",
    "    tree = ET.parse(fileName)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for s in root.iter('s'): # every sentence is found with <s> tags\n",
    "    \n",
    "        textFull.append('<s>')\n",
    "    \n",
    "        for elem in s.findall('*'):\n",
    "\n",
    "            if elem.text != None:    \n",
    "                textFull.append(elem.text)\n",
    "                \n",
    "        textFull.append('</s>')\n",
    "        \n",
    "    \n",
    "print(\"Word count: \",len(textFull))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing split\n",
    "\n",
    "Splitting data using an 80/20 ratio and check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 422 duplicate sentences from test data\n",
      "\n",
      "Training data\t Word count: 939306 \tUnique words count: 68898\n",
      "Testing data\t Word count: 230057 \tUnique words count: 31352\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into sentences within a list\n",
    "sentences = []\n",
    "\n",
    "for i, word in enumerate(textFull):\n",
    "    if word == \"<s>\":\n",
    "        idx = i\n",
    "    elif word == \"</s>\":\n",
    "        sentences.append(textFull[idx:i+1])\n",
    "\n",
    "# shuffle to randomise the order of the sentences. using seed to make sure it's always the same\n",
    "random.seed(42)\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# split the list of sentences into 80-20\n",
    "split = int(len(sentences) * 0.8)\n",
    "\n",
    "trainDataSentences = sentences[:split]\n",
    "testDataWithDupesSentences = sentences[split:]\n",
    "\n",
    "#to make sure there are no duplicates\n",
    "uniqueSentences = set() \n",
    "for sentence in trainDataSentences: #getting all unique sentences in the training data\n",
    "    uniqueSentences.add(tuple(sentence))\n",
    "\n",
    "testDataSentences = []\n",
    "duplicateCount = 0\n",
    "for sentence in testDataWithDupesSentences: #checking if sentences in test data is found in training data\n",
    "    if tuple(sentence) not in uniqueSentences:\n",
    "        testDataSentences.append(sentence)\n",
    "    else:\n",
    "        duplicateCount += 1\n",
    "\n",
    "print(\"Removed\", duplicateCount, \"duplicate sentences from test data\\n\")\n",
    "\n",
    "#flattening lists\n",
    "trainData = [word for sentence in trainDataSentences for word in sentence]\n",
    "testData = [word for sentence in testDataSentences for word in sentence]\n",
    "\n",
    "print(\"Training data\\t Word count:\", len(trainData), \"\\tUnique words count:\",len(set(trainData)))\n",
    "print(\"Testing data\\t Word count:\", len(testData), \"\\tUnique words count:\",len(set(testData)))\n",
    "\n",
    "# writing the training and testing sets to files for easy access\n",
    "# with open(\"dataset/Training_Set.json\", \"w\") as f:\n",
    "#     json.dump(trainData, f)\n",
    "\n",
    "# with open(\"dataset/Testing_Set.json\", \"w\") as f:\n",
    "#     json.dump(testDataSentences, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "\n",
    "with open(\"dataset/Training_Set.json\", \"r\") as f:\n",
    "    trainData = json.load(f)\n",
    "\n",
    "with open(\"dataset/Testing_Set.json\", \"r\") as f:\n",
    "    testData = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNK Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFreqs = {}\n",
    "\n",
    "for word in trainData: #taking count of each word\n",
    "    if word not in wordFreqs:\n",
    "        wordFreqs[word] = 1\n",
    "    else:\n",
    "        wordFreqs[word] += 1\n",
    "\n",
    "trainDataUNK = []\n",
    "\n",
    "for word in trainData: #if word appears 2 or less times it is appended as a <UNK> token\n",
    "    if wordFreqs[word] <= 2:\n",
    "       trainDataUNK.append(\"<UNK>\")\n",
    "    else:\n",
    "        trainDataUNK.append(word)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(text, n): #n is size of ngrams\n",
    "\n",
    "    ngramCounts = {} #dict to store ngrams\n",
    "\n",
    "    for i in range(len(text)-n+1): #len(text)-n+1 is the number of all possible ngrams\n",
    "\n",
    "        ngram = tuple(text[i:i+n]) #creates ngram\n",
    "\n",
    "        if ngram in ngramCounts: #check to see if it already exists\n",
    "            ngramCounts[ngram] += 1\n",
    "        else:\n",
    "            ngramCounts[ngram] = 1\n",
    "\n",
    "    return ngramCounts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing N-Gram function with a test string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:\n",
      " {('<s>',): 3, ('I',): 3, ('am',): 2, ('Sam',): 2, ('</s>',): 3, ('do',): 1, ('not',): 1, ('like',): 1, ('green',): 1, ('eggs',): 1, ('and',): 1, ('ham',): 1} \n",
      "\n",
      "Bigram:\n",
      " {('<s>', 'I'): 2, ('I', 'am'): 2, ('am', 'Sam'): 1, ('Sam', '</s>'): 1, ('</s>', '<s>'): 2, ('<s>', 'Sam'): 1, ('Sam', 'I'): 1, ('am', '</s>'): 1, ('I', 'do'): 1, ('do', 'not'): 1, ('not', 'like'): 1, ('like', 'green'): 1, ('green', 'eggs'): 1, ('eggs', 'and'): 1, ('and', 'ham'): 1, ('ham', '</s>'): 1} \n",
      "\n",
      "Trigram:\n",
      " {('<s>', 'I', 'am'): 1, ('I', 'am', 'Sam'): 1, ('am', 'Sam', '</s>'): 1, ('Sam', '</s>', '<s>'): 1, ('</s>', '<s>', 'Sam'): 1, ('<s>', 'Sam', 'I'): 1, ('Sam', 'I', 'am'): 1, ('I', 'am', '</s>'): 1, ('am', '</s>', '<s>'): 1, ('</s>', '<s>', 'I'): 1, ('<s>', 'I', 'do'): 1, ('I', 'do', 'not'): 1, ('do', 'not', 'like'): 1, ('not', 'like', 'green'): 1, ('like', 'green', 'eggs'): 1, ('green', 'eggs', 'and'): 1, ('eggs', 'and', 'ham'): 1, ('and', 'ham', '</s>'): 1}\n"
     ]
    }
   ],
   "source": [
    "testString = '<s> I am Sam </s> <s> Sam I am </s> <s> I do not like green eggs and ham </s>'\n",
    "testList = list(testString.split(\" \"))\n",
    "\n",
    "print(\"Unigram:\\n\",ngram(testList,1), \"\\n\")\n",
    "print(\"Bigram:\\n\",ngram(testList,2), \"\\n\")\n",
    "print(\"Trigram:\\n\",ngram(testList,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "#### Probability Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnigramModel(dataset):\n",
    "\n",
    "    ngrams = ngram(dataset, 1)\n",
    "\n",
    "    vanillaModel = {} # where vanilla probabilities will be stored\n",
    "    smoothModel = {} # where laplace smoothing probabilities will be stored\n",
    "\n",
    "    totalWords = len(dataset) # number of words in dataset\n",
    "    vocabulary = len(set(dataset)) # number of unique words in dataset\n",
    "    \n",
    "    for ngram1 in ngrams:\n",
    "        \n",
    "        vanillaModel[ngram1] = ngrams[ngram1]/totalWords # calculating probability\n",
    "        smoothModel[ngram1] = (ngrams[ngram1]+1)/(totalWords+vocabulary) # calculating probability with laplace smoothing\n",
    "\n",
    "    return vanillaModel, smoothModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BigramModel(dataset):\n",
    "\n",
    "    ngrams = ngram(dataset, 2)\n",
    "\n",
    "    vanillaModel = {}\n",
    "    smoothModel = {}\n",
    "\n",
    "    vocabulary = len(set(dataset))\n",
    "    \n",
    "    for ngram1 in ngrams:\n",
    "        \n",
    "        count = 0\n",
    "        prefix = ngram1[0] \n",
    "\n",
    "        for ngram2 in ngrams:\n",
    "            \n",
    "            if prefix in ngram2:\n",
    "                count += 1\n",
    "                \n",
    "        vanillaModel[ngram1] = ngrams[ngram1]/count # calculating probability\n",
    "        smoothModel[ngram1] = (ngrams[ngram1]+1)/(count+vocabulary) # calculating probability with laplace smoothing\n",
    "\n",
    "    return vanillaModel, smoothModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrigramModel(dataset):\n",
    "\n",
    "    ngrams = ngram(dataset, 3)\n",
    "\n",
    "    vanillaModel = {}\n",
    "    smoothModel = {}\n",
    "\n",
    "    vocabulary = len(set(dataset))\n",
    "    \n",
    "    for ngram1 in ngrams:\n",
    "\n",
    "        count = 0\n",
    "        prefix = ngram1[0:2]\n",
    "\n",
    "        for ngram2 in ngrams:\n",
    "            \n",
    "            if prefix in zip(ngram2, ngram2[1:]):\n",
    "                count += 1\n",
    "            \n",
    "        vanillaModel[ngram1] = ngrams[ngram1]/count # calculating probability\n",
    "        smoothModel[ngram1] = (ngrams[ngram1]+1)/(count+vocabulary) # calculating probability with laplace smoothing\n",
    "        \n",
    "    return vanillaModel, smoothModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving/Loading functions for JSON files\n",
    "\n",
    "Used since JSON files do not accept tuples as keys\n",
    "\n",
    "Source: https://stackoverflow.com/questions/7001606/json-serialize-a-dictionary-with-tuples-as-key?answertab=scoredesc#tab-top\n",
    "\n",
    "- saveJSON<br>\n",
    "Converts the tuples in the keys to strings and saves to JSON\n",
    "\n",
    "- loadJSON<br>\n",
    "Converts strings back into tuples and returns dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveJSON(dictionary, filename):\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        \n",
    "        # changes keys to string\n",
    "        k = dictionary.keys()\n",
    "        v = dictionary.values()\n",
    "        kstrings = [str(i) for i in k]\n",
    "\n",
    "        json.dump(json.dumps(dict(zip(*[kstrings,v]))),f)\n",
    "        print(\"File was saved\")\n",
    "\n",
    "def loadJSON(filename):\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "    \n",
    "        data = json.load(f)\n",
    "        dictionary = json.loads(data)\n",
    "\n",
    "        # converts back to tuples\n",
    "        k = dictionary.keys() \n",
    "        v = dictionary.values() \n",
    "        k1 = [eval(i) for i in k] \n",
    "        \n",
    "        return dict(zip(*[k1,v])) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanillaUnigramModel, laplaceUnigramModel = UnigramModel(trainData) \n",
    "# saveJSON(vanillaUnigramModel, \"models/Vanilla_Unigram.json\")\n",
    "# saveJSON(laplaceUnigramModel, \"models/Laplace_Unigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNKUnigramModel, _ = UnigramModel(trainDataUNK)\n",
    "# saveJSON(UNKUnigramModel, \"models/UNK_Unigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanillaBigramModel, laplaceBigramModel = BigramModel(trainData)\n",
    "# saveJSON(vanillaBigramModel, \"models/Vanilla_Bigram.json\")\n",
    "# saveJSON(laplaceBigramModel, \"models/Laplace_Bigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNKBigramModel, _ = BigramModel(trainDataUNK)\n",
    "# saveJSON(UNKBigramModel, \"models/UNK_Bigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanillaTrigramModel, laplaceTrigramModel = TrigramModel(trainData) \n",
    "# saveJSON(vanillaTrigramModel, \"models/Vanilla_Trigram.json\")\n",
    "# saveJSON(laplaceTrigramModel, \"models/Laplace_Trigram.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNKTrigramModel, _ = TrigramModel(trainDataUNK)\n",
    "# saveJSON(UNKTrigramModel, \"models/UNK_Trigram.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanillaUnigramModel = loadJSON(\"full-models/Vanilla_Unigram.json\")\n",
    "vanillaBigramModel = loadJSON(\"full-models/Vanilla_Bigram.json\")\n",
    "vanillaTrigramModel = loadJSON(\"full-models/Vanilla_Trigram.json\")\n",
    "\n",
    "laplaceUnigramModel = loadJSON(\"full-models/Laplace_Unigram.json\")\n",
    "laplaceBigramModel = loadJSON(\"full-models/Laplace_Bigram.json\")\n",
    "laplaceTrigramModel = loadJSON(\"full-models/Laplace_Trigram.json\")\n",
    "\n",
    "UNKUnigramModel = loadJSON(\"full-models/UNK_Unigram.json\")\n",
    "UNKBigramModel = loadJSON(\"full-models/UNK_Bigram.json\")\n",
    "UNKTrigramModel = loadJSON(\"full-models/UNK_Trigram.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokeniseSentence(sentenceString):\n",
    "\n",
    "    sentence = re.findall(r'\\b\\w+\\b|<\\/?s>|[^\\w\\s]', sentenceString)\n",
    "    \n",
    "    sentence.insert(0, '<s>')\n",
    "    sentence.append('</s>')\n",
    "\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence = \"Baker expressed grave concern at the Bootle killing and attacked various institutions for the fact that society has become more violent and selfish.\"\n",
    "testSentence = \"That is all Botham wants to hear.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VanillaUnigramProbabilities(sentence):\n",
    "    \n",
    "    unigramProbabilties = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        \n",
    "        probability = vanillaUnigramModel.get((word,), 0) # gets probability of word, if word not found 0 is returned\n",
    "        \n",
    "        unigramProbabilties.append(probability)\n",
    "        \n",
    "    return unigramProbabilties\n",
    "\n",
    "def VanillaBigramProbabilties(sentence):\n",
    "    \n",
    "    bigramProbabilities = []\n",
    "    \n",
    "    for i in range(len(sentence)-1): # -1 so it doesnt go out of bounds\n",
    "        \n",
    "        bigram = tuple(sentence[i:i+2])\n",
    "        \n",
    "        probability = vanillaBigramModel.get((bigram), 0)\n",
    "        bigramProbabilities.append(probability)\n",
    "        \n",
    "    return bigramProbabilities\n",
    "\n",
    "def VanillaTrigramProbabilties(sentence):\n",
    "    \n",
    "    trigramProbabilities = []\n",
    "        \n",
    "    for i in range(len(sentence)-2): # -2 so it doesnt go out of bounds\n",
    "        \n",
    "        trigram = tuple(sentence[i:i+3])\n",
    "        \n",
    "        probability = vanillaTrigramModel.get((trigram), 0)\n",
    "        trigramProbabilities.append(probability)\n",
    "        \n",
    "    return trigramProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = len(set(trainData))\n",
    "\n",
    "def LaplaceUnigramProbabilities(sentence):\n",
    "    \n",
    "    unigramProbabilties = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        \n",
    "        probability = laplaceUnigramModel.get((word,), (1/vocabulary)) # gets probability of word, if word not found 1/vocab is returned\n",
    "        \n",
    "        unigramProbabilties.append(probability)\n",
    "        \n",
    "    return unigramProbabilties\n",
    "\n",
    "def LaplaceBigramProbabilities(sentence):\n",
    "   \n",
    "    bigramProbabilities = []\n",
    "    \n",
    "    for i in range(len(sentence)-1): # -1 so it doesnt go out of bounds\n",
    "        \n",
    "        bigram = tuple(sentence[i:i+2])\n",
    "        \n",
    "        probability = laplaceBigramModel.get((bigram), (1/vocabulary))\n",
    "        bigramProbabilities.append(probability)\n",
    "        \n",
    "    return bigramProbabilities\n",
    "\n",
    "def LaplaceTrigramProbabilities(sentence):\n",
    "    \n",
    "    trigramProbabilities = []\n",
    "        \n",
    "    for i in range(len(sentence)-2): # -2 so it doesnt go out of bounds\n",
    "        \n",
    "        trigram = tuple(sentence[i:i+3])\n",
    "        \n",
    "        probability = laplaceTrigramModel.get((trigram), (1/vocabulary))\n",
    "        trigramProbabilities.append(probability)\n",
    "        \n",
    "    return trigramProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNKUnigramProbabilities(sentence):\n",
    "    \n",
    "    unigramProbabilties = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        \n",
    "        probability = UNKUnigramModel.get((word,), UNKUnigramModel[('<UNK>',)]) # gets probability of word, if word not found prob of <UNK> is returned\n",
    "        \n",
    "        unigramProbabilties.append(probability)\n",
    "        \n",
    "    return unigramProbabilties\n",
    "\n",
    "def UNKBigramProbabilities(sentence):\n",
    "    \n",
    "    bigramProbabilities = []\n",
    "    \n",
    "    for i in range(len(sentence)-1): # -1 so it doesnt go out of bounds\n",
    "        \n",
    "        bigram = tuple(sentence[i:i+2])\n",
    "        \n",
    "        probability = UNKBigramModel.get((bigram), UNKBigramModel[('<UNK>', '<UNK>')])\n",
    "        bigramProbabilities.append(probability)\n",
    "        \n",
    "    return bigramProbabilities\n",
    "\n",
    "def UNKTrigramProbabilities(sentence):\n",
    "    \n",
    "    trigramProbabilities = []\n",
    "        \n",
    "    for i in range(len(sentence)-2): # -2 so it doesnt go out of bounds\n",
    "        \n",
    "        trigram = tuple(sentence[i:i+3])\n",
    "        \n",
    "        probability = UNKTrigramModel.get((trigram), UNKTrigramModel[('<UNK>', '<UNK>', '<UNK>')])\n",
    "        trigramProbabilities.append(probability)\n",
    "        \n",
    "    return trigramProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProbabilityTotal(probabilities): #gets list of probabilties and multiplies them\n",
    "    \n",
    "    sentenceProbability = 1\n",
    "    \n",
    "    for probability in probabilities:\n",
    "        sentenceProbability *= probability\n",
    "        \n",
    "    return sentenceProbability\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sen_Probability(sentence):\n",
    "\n",
    "    tokenisedSentence = TokeniseSentence(sentence)\n",
    "\n",
    "    print(\"Vanilla Unigram:\", ProbabilityTotal(VanillaUnigramProbabilities(tokenisedSentence)))\n",
    "    print(\"Vanilla Bigram:\", ProbabilityTotal(VanillaBigramProbabilties(tokenisedSentence)))\n",
    "    print(\"Vanilla Trigram:\", ProbabilityTotal(VanillaTrigramProbabilties(tokenisedSentence)),\"\\n\")\n",
    "\n",
    "    print(\"Laplace Unigram:\", ProbabilityTotal(LaplaceUnigramProbabilities(tokenisedSentence)))\n",
    "    print(\"Laplace Bigram:\", ProbabilityTotal(LaplaceBigramProbabilities(tokenisedSentence)))\n",
    "    print(\"Laplace Trigram:\", ProbabilityTotal(LaplaceTrigramProbabilities(tokenisedSentence)), \"\\n\")\n",
    "\n",
    "    print(\"UNK Unigram:\", ProbabilityTotal(UNKUnigramProbabilities(tokenisedSentence)))\n",
    "    print(\"UNK Bigram:\", ProbabilityTotal(UNKBigramProbabilities(tokenisedSentence)))\n",
    "    print(\"UNK Trigram:\", ProbabilityTotal(UNKTrigramProbabilities(tokenisedSentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Unigram: 0.0\n",
      "Vanilla Bigram: 0\n",
      "Vanilla Trigram: 0 \n",
      "\n",
      "Laplace Unigram: 7.935829365590901e-50\n",
      "Laplace Bigram: 4.1489084131430815e-49\n",
      "Laplace Trigram: 2.8585149184873204e-44 \n",
      "\n",
      "UNK Unigram: 8.89511311306738e-34\n",
      "UNK Bigram: 5.8992170207896055e-06\n",
      "UNK Trigram: 2.6897377498946083e-07\n"
     ]
    }
   ],
   "source": [
    "Sen_Probability(\"The quick brown fox jumped over the lazy dog\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramWeight = 0.1\n",
    "bigramWeight = 0.3\n",
    "trigramWeight = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VanillaLinearInterpolation(sentence):\n",
    "    \n",
    "    unigramProbabilities = VanillaUnigramProbabilities(sentence)\n",
    "    \n",
    "    bigramProbabilities = VanillaBigramProbabilties(sentence)\n",
    "    \n",
    "    trigramProbabilities = VanillaTrigramProbabilties(sentence)\n",
    "        \n",
    "    weightedProbabilities = []\n",
    "        \n",
    "    for p1, p2, p3 in zip(unigramProbabilities, bigramProbabilities, trigramProbabilities):\n",
    "        weightedProbability = unigramWeight*p1 + bigramWeight*p2 + trigramWeight*p3\n",
    "        \n",
    "        weightedProbabilities.append(weightedProbability)\n",
    "        \n",
    "    return weightedProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LaplaceSmoothingLinearInterpolation(sentence):\n",
    "    \n",
    "    unigramProbabilties = LaplaceUnigramProbabilities(sentence)\n",
    "    \n",
    "    bigramProbabilities = LaplaceBigramProbabilities(sentence)\n",
    "        \n",
    "    trigramProbabilities = LaplaceTrigramProbabilities(sentence)\n",
    "        \n",
    "    weightedProbabilities = []\n",
    "        \n",
    "    for p1, p2, p3 in zip(unigramProbabilties, bigramProbabilities, trigramProbabilities):\n",
    "        weightedProbability = unigramWeight*p1 + bigramWeight*p2 + trigramWeight*p3\n",
    "        \n",
    "        weightedProbabilities.append(weightedProbability)\n",
    "        \n",
    "    return weightedProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNKLinearInterpolation(sentence):\n",
    "    \n",
    "    unigramProbabilties = UNKUnigramProbabilities(sentence)\n",
    "    \n",
    "    bigramProbabilities = UNKBigramProbabilities(sentence)\n",
    "        \n",
    "    trigramProbabilities = UNKTrigramProbabilities(sentence)\n",
    "        \n",
    "    weightedProbabilities = []\n",
    "        \n",
    "    for p1, p2, p3 in zip(unigramProbabilties, bigramProbabilities, trigramProbabilities):\n",
    "        weightedProbability = unigramWeight*p1 + bigramWeight*p2 + trigramWeight*p3\n",
    "        \n",
    "        weightedProbabilities.append(weightedProbability)\n",
    "        \n",
    "    return weightedProbabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Interpolation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Baker expressed grave concern at the Bootle killing and attacked various institutions for the fact that society has become more violent and selfish. \n",
      "\n",
      "Vanilla Linear Interpolation: 1.9200583008021034e-133\n",
      "Laplace Smoothing Linear Interpolation: 8.154036197208486e-113\n",
      "UNK Linear Interpolation: 5.864328681572975e-17 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence: \", trainSentence, \"\\n\")\n",
    "tokenisedSentence = TokeniseSentence(trainSentence)\n",
    "\n",
    "print(\"Vanilla Linear Interpolation:\", ProbabilityTotal(VanillaLinearInterpolation(tokenisedSentence)))\n",
    "print(\"Laplace Smoothing Linear Interpolation:\", ProbabilityTotal(LaplaceSmoothingLinearInterpolation(tokenisedSentence)))\n",
    "print(\"UNK Linear Interpolation:\", ProbabilityTotal(UNKLinearInterpolation(tokenisedSentence)),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  That is all Botham wants to hear. \n",
      "\n",
      "Vanilla Linear Interpolation: 5.441195234524026e-36\n",
      "Laplace Smoothing Linear Interpolation: 4.855468094351593e-36\n",
      "UNK Linear Interpolation: 3.0123521199581038e-06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence: \", testSentence, \"\\n\")\n",
    "tokenisedSentence = TokeniseSentence(testSentence)\n",
    "\n",
    "print(\"Vanilla Linear Interpolation:\", ProbabilityTotal(VanillaLinearInterpolation(tokenisedSentence)))\n",
    "print(\"Laplace Smoothing Linear Interpolation:\", ProbabilityTotal(LaplaceSmoothingLinearInterpolation(tokenisedSentence)))\n",
    "print(\"UNK Linear Interpolation:\", ProbabilityTotal(UNKLinearInterpolation(tokenisedSentence)),\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results:\n",
      "\n",
      "Vanilla Unigram: Infinite\n",
      "Vanilla Bigram: Infinite\n",
      "Vanilla Trigram: Infinite \n",
      "\n",
      "Laplace Smoothing Unigram: 48961.87463134319\n",
      "Laplace Smoothing Bigram: 7328.465198230634\n",
      "Laplace Smoothing Trigram: 919.3839666976847 \n",
      "\n",
      "UNK Unigram: 153.71380302325178\n",
      "UNK Bigram: 2.691244162146465\n",
      "UNK Trigram: 2.800004397266389 \n",
      "\n",
      "Vanilla Linear Interpolation: Infinite\n",
      "Laplace Smoothing Linear Interpolation: 920.9185826564983\n",
      "UNK Linear Interpolation: 1.0000027503082094\n"
     ]
    }
   ],
   "source": [
    "vanillaUnigram = vanillaBigram = vanillaTrigram = 0\n",
    "laplaceUnigram = laplaceBigram = laplaceTrigram = 0\n",
    "unkUnigram = unkBigram = unkTrigram = 0\n",
    "vanillaLinearInterpolation = laplaceLinearInterpolation = unkLinearInterpolation = 0\n",
    "\n",
    "for sentence in testData:\n",
    "\n",
    "    vanillaUnigramNum = ProbabilityTotal(VanillaUnigramProbabilities(sentence))\n",
    "    vanillaUnigram += math.log(vanillaUnigramNum) if vanillaUnigramNum != 0 else -1e9\n",
    "\n",
    "    vanillaBigramNum = ProbabilityTotal(VanillaBigramProbabilties(sentence))\n",
    "    vanillaBigram += math.log(vanillaBigramNum) if vanillaBigramNum != 0 else -1e9\n",
    "    \n",
    "    vanillaTrigramNum = ProbabilityTotal(VanillaTrigramProbabilties(sentence))\n",
    "    vanillaTrigram += math.log(vanillaTrigramNum) if vanillaTrigramNum != 0 else -1e9\n",
    "\n",
    "    laplaceUnigramNum = ProbabilityTotal(LaplaceUnigramProbabilities(sentence))\n",
    "    laplaceUnigram += math.log(laplaceUnigramNum) if laplaceUnigramNum != 0 else -1e9\n",
    "    \n",
    "    laplaceBigramNum = ProbabilityTotal(LaplaceBigramProbabilities(sentence))\n",
    "    laplaceBigram += math.log(laplaceBigramNum) if laplaceBigramNum != 0 else -1e9\n",
    "\n",
    "    laplaceTrigramNum = ProbabilityTotal(LaplaceTrigramProbabilities(sentence))\n",
    "    laplaceTrigram += math.log(laplaceTrigramNum) if laplaceTrigramNum != 0 else -1e9\n",
    "\n",
    "    unkUnigramNum = ProbabilityTotal(UNKUnigramProbabilities(sentence))\n",
    "    unkUnigram += math.log(unkUnigramNum) if unkUnigramNum != 0 else -1e9\n",
    "\n",
    "    unkBigramNum = ProbabilityTotal(UNKBigramProbabilities(sentence))\n",
    "    unkBigram += math.log(unkBigramNum) if unkBigramNum != 0 else -1e9\n",
    "\n",
    "    unkTrigramNum = ProbabilityTotal(UNKTrigramProbabilities(sentence))\n",
    "    unkTrigram += math.log(unkTrigramNum) if unkTrigramNum != 0 else -1e9\n",
    "    \n",
    "    vanillaLinearInterpolationNum = ProbabilityTotal(VanillaLinearInterpolation(sentence))\n",
    "    vanillaLinearInterpolation += math.log(vanillaLinearInterpolationNum) if vanillaLinearInterpolationNum != 0 else -1e9\n",
    "\n",
    "    laplaceLinearInterpolationNum = ProbabilityTotal(LaplaceSmoothingLinearInterpolation(sentence))\n",
    "    laplaceLinearInterpolation += math.log(laplaceLinearInterpolationNum) if laplaceLinearInterpolationNum != 0 else -1e9\n",
    "    \n",
    "    unkLinearInterpolationNum = ProbabilityTotal(UNKLinearInterpolation(sentence))\n",
    "    unkLinearInterpolation = math.log(unkLinearInterpolationNum) if unkLinearInterpolationNum != 0 else -1e9\n",
    "\n",
    "wordCount = len([word for sentence in testData for word in sentence])\n",
    "\n",
    "try:\n",
    "    vanillaUnigram_perplexity = math.exp(-vanillaUnigram / wordCount)\n",
    "except:\n",
    "    vanillaUnigram_perplexity = 'Infinite'\n",
    "    \n",
    "try:\n",
    "    vanillaBigram_perplexity = math.exp(-vanillaBigram / wordCount)\n",
    "except:\n",
    "    vanillaBigram_perplexity = 'Infinite'\n",
    "    \n",
    "try:\n",
    "    vanillaTrigram_perplexity = math.exp(-vanillaTrigram / wordCount)\n",
    "except:\n",
    "    vanillaTrigram_perplexity = 'Infinite'\n",
    "    \n",
    "try:\n",
    "    laplaceUnigram_perplexity = math.exp(-laplaceUnigram / wordCount)\n",
    "except:\n",
    "    laplaceUnigram_perplexity = 'Infinite'\n",
    "\n",
    "try:\n",
    "    laplaceBigram_perplexity = math.exp(-laplaceBigram / wordCount)\n",
    "except:\n",
    "    laplaceBigram_perplexity = 'Infinite'\n",
    "\n",
    "try:\n",
    "    laplaceTrigram_perplexity = math.exp(-laplaceTrigram / wordCount)\n",
    "except:\n",
    "    laplaceTrigram_perplexity = 'Infinite'\n",
    "\n",
    "try:\n",
    "    unkUnigram_perplexity = math.exp(-unkUnigram / wordCount)\n",
    "except:\n",
    "    unkUnigram_perplexity = 'Infinite'\n",
    "\n",
    "try:\n",
    "    unkBigram_perplexity = math.exp(-unkBigram / wordCount)\n",
    "except:\n",
    "    unkBigram_perplexity = 'Infinite'\n",
    "\n",
    "try:\n",
    "    unkTrigram_perplexity = math.exp(-unkTrigram / wordCount)\n",
    "except:\n",
    "    unkTrigram_perplexity = 'Infinite'\n",
    "\n",
    "try:\n",
    "    vanillaLinearInterpolation_perplexity = math.exp(-vanillaLinearInterpolation / wordCount)\n",
    "except:\n",
    "    vanillaLinearInterpolation_perplexity = 'Infinite'\n",
    "\n",
    "try:\n",
    "    laplaceLinearInterpolation_perplexity = math.exp(-laplaceLinearInterpolation / wordCount)\n",
    "except:\n",
    "    laplaceLinearInterpolation_perplexity = 'Infinite'\n",
    "\n",
    "try:\n",
    "    unkLinearInterpolation_perplexity = math.exp(-unkLinearInterpolation / wordCount)\n",
    "except:\n",
    "    unkLinearInterpolation_perplexity = 'Infinite'\n",
    "\n",
    "print(\"Perplexity results:\\n\")\n",
    "\n",
    "print(\"Vanilla Unigram:\", vanillaUnigram_perplexity)\n",
    "print(\"Vanilla Bigram:\", vanillaBigram_perplexity)\n",
    "print(\"Vanilla Trigram:\", vanillaTrigram_perplexity,\"\\n\")\n",
    "\n",
    "print(\"Laplace Smoothing Unigram:\", laplaceUnigram_perplexity)\n",
    "print(\"Laplace Smoothing Bigram:\", laplaceBigram_perplexity)\n",
    "print(\"Laplace Smoothing Trigram:\", laplaceTrigram_perplexity,\"\\n\")\n",
    "\n",
    "print(\"UNK Unigram:\", unkUnigram_perplexity)\n",
    "print(\"UNK Bigram:\", unkBigram_perplexity)\n",
    "print(\"UNK Trigram:\", unkTrigram_perplexity,\"\\n\")\n",
    "\n",
    "print(\"Vanilla Linear Interpolation:\", vanillaLinearInterpolation_perplexity)\n",
    "print(\"Laplace Smoothing Linear Interpolation:\", laplaceLinearInterpolation_perplexity)\n",
    "print(\"UNK Linear Interpolation:\", unkLinearInterpolation_perplexity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla:\n",
      "This were yearsafer a finest donerealise\n",
      "\n",
      "Laplace:\n",
      "This wanted partedtheir plastic £30,000a-year water We her , in microcomputer give , John with (visit centre undisputed used popular Cross-Country slash four West manly to garage £1.5 their Greenpeace towards trauma , has year in .£5 high Towards by position withe message more always goes The elation higherYoungof second the parentship a stay bowling \n",
      "\n",
      "UNK:\n",
      "This a stopped quite \n"
     ]
    }
   ],
   "source": [
    "def generateSentenceUnigram(unigramModel, word):\n",
    "    \n",
    "    startTag = '<s>'\n",
    "    endTag = '</s>'\n",
    "    \n",
    "    sentence = [startTag, word]\n",
    "    \n",
    "    while(word != endTag):\n",
    "\n",
    "        total = sum(value for value in unigramModel.values())\n",
    "        probs = [value / total for value in unigramModel.values()]\n",
    "\n",
    "        word = random.choices(list(unigramModel.keys()), probs)[0][0]\n",
    "\n",
    "        while True:\n",
    "            word = random.choices(list(unigramModel.keys()), probs)[0][0]\n",
    "            if((word != '<UNK>') and (word != '<s>')): #to handle unk model\n",
    "                break\n",
    "    \n",
    "        \n",
    "        sentence.append(word)\n",
    "    \n",
    "    sentence.remove('<s>')\n",
    "    sentence.remove('</s>')\n",
    "\n",
    "    print(''.join(sentence))\n",
    "\n",
    "\n",
    "print(\"Vanilla:\")\n",
    "generateSentenceUnigram(vanillaUnigramModel, \"This \")\n",
    "print(\"\\nLaplace:\")\n",
    "generateSentenceUnigram(laplaceUnigramModel, \"This \")\n",
    "print(\"\\nUNK:\")\n",
    "generateSentenceUnigram(UNKUnigramModel, \"This \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla:\n",
      "This would represent Hampshire Ratings coordinator Joan Andrews\n",
      "\n",
      "Laplace:\n",
      "This was the brakes are declining rapidly, who had published today saying that New York became the Tories\n",
      "\n",
      "UNK:\n",
      "This includes communal living near Ipswich's Cup for libel laws.\n"
     ]
    }
   ],
   "source": [
    "def generateSentenceBigram(bigramModel, word):\n",
    "\n",
    "    startTag = '<s>'\n",
    "    endTag = '</s>'\n",
    "\n",
    "    sentence = [startTag, word]\n",
    "    chosenNgram = tuple()\n",
    "\n",
    "    while (word != endTag): #loop until endTag is found\n",
    "\n",
    "        startsWithWord = [(key,value) for key, value in bigramModel.items() if key[0] == word] #finds all bigrams that start with current word\n",
    "        \n",
    "        #calculating probabilties for each bigram\n",
    "        total = sum(value for _, value in startsWithWord)\n",
    "        probs = [value / total for _, value in startsWithWord]\n",
    "\n",
    "        #choosing a random bigram according to its probability\n",
    "        while True:\n",
    "            chosenNgram = random.choices(startsWithWord, probs)[0][0]\n",
    "            if('<UNK>' not in chosenNgram): #to handle unk model\n",
    "                break\n",
    "        \n",
    "        #setting the next word\n",
    "        word = chosenNgram[1]\n",
    "\n",
    "        sentence.append(word)\n",
    "\n",
    "    sentence.remove('<s>')\n",
    "    sentence.remove('</s>')\n",
    "   \n",
    "    print(''.join(sentence))\n",
    "\n",
    "print(\"Vanilla:\")  \n",
    "generateSentenceBigram(vanillaBigramModel, \"This \")\n",
    "print(\"\\nLaplace:\")\n",
    "generateSentenceBigram(laplaceBigramModel, \"This \")\n",
    "print(\"\\nUNK:\")\n",
    "generateSentenceBigram(UNKBigramModel, \"This \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla:\n",
      "This summer its market leadership while falling short of blaming BR or the lack of success for fund-holders' own brands, ranging from bullying by classmates to schoolwork anxiety.\n",
      "\n",
      "Laplace:\n",
      "This always proves to be made in the area and encourage local people must be changed to four men and 14 for first quarter of new recordings issued recently; not surprisingly perhaps, ignored fellow Olympic bronze medalist Denis Stewart's decision to implement community care was distributed to customers, but it has taken her up in a freak holiday accident was undoubtedly the wiring errors in recent years.\n",
      "\n",
      "UNK:\n",
      "This was blocked, Phil Whelan drove the amount owned to investors; while the proportion of female sperm whales trapped in debt\n"
     ]
    }
   ],
   "source": [
    "def generateSentenceTrigram(trigramModel, word):\n",
    "\n",
    "    startTag = '<s>'\n",
    "    endTag = '</s>'\n",
    "\n",
    "    sentence = [startTag]\n",
    "\n",
    "    startsWithTag = [(key, value) for key, value in trigramModel.items() if key[0] == startTag and key[1] == word] #find all trigrams that start with tag and word\n",
    "    total = sum(value for _, value in startsWithTag)\n",
    "    probs = [value / total for _, value in startsWithTag]\n",
    "\n",
    "    while True:\n",
    "        chosenNgram = random.choices(startsWithTag, probs)[0][0]\n",
    "        if('<UNK>' not in chosenNgram):\n",
    "            break\n",
    "\n",
    "    word = chosenNgram[1] #set current word as last word in bigram\n",
    "    bigram = (chosenNgram[1], chosenNgram[2]) #set bigram as last two words in trigram\n",
    "\n",
    "    sentence.append(word)\n",
    "\n",
    "    while(word != endTag):\n",
    "\n",
    "        startsWithBigram = [(key, value) for key, value in trigramModel.items() if key[:2] == bigram] #find all trigrams that start with current bigram\n",
    "\n",
    "        total = sum(value for _, value in startsWithBigram)\n",
    "        probs = [value / total for _, value in startsWithBigram]\n",
    "\n",
    "        while True:\n",
    "            chosenNgram = random.choices(startsWithBigram, probs)[0][0] #pick according to probability\n",
    "            if('<UNK>' not in chosenNgram): #to handle unk model\n",
    "                break\n",
    "\n",
    "        word = chosenNgram[1]\n",
    "        bigram = (chosenNgram[1], chosenNgram[2])\n",
    "        \n",
    "        sentence.append(word)\n",
    "\n",
    "    sentence.remove('<s>')\n",
    "    sentence.remove('</s>')\n",
    "    \n",
    "    print(''.join(sentence))\n",
    "\n",
    "print(\"Vanilla:\")\n",
    "generateSentenceTrigram(vanillaTrigramModel, \"This \")\n",
    "print(\"\\nLaplace:\")\n",
    "generateSentenceTrigram(laplaceTrigramModel, \"This \")\n",
    "print(\"\\nUNK:\")\n",
    "generateSentenceTrigram(UNKTrigramModel, \"This \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e556872412c9668e080d844f72a9aacd3b42500e70ed86345b6212c46b5b8406"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
